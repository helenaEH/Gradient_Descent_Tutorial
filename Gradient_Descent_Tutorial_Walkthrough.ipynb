{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "from random import randint\n",
    "from IPython.display import HTML\n",
    "import imageio\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# turn off RuntimeWarning \n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create data\n",
    "Create a continuous set of X data  \n",
    "Following fuction uses list comprehension to create 2000 random datapoints that are between 0.01 and 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrue = [randint(1,100) * 0.01 for x in range(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create y data on the back of the data with noise   \n",
    "### Linear function: $f(x) = a + bx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-75d729a5bd8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mINTERCEPT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNOISE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mytrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSLOPE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mINTERCEPT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNOISE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mXtrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# np.random.normal skews our data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xtrue' is not defined"
     ]
    }
   ],
   "source": [
    "SLOPE = 2.0\n",
    "INTERCEPT = -1.0\n",
    "NOISE = 0.2\n",
    "ytrue = [SLOPE * x + INTERCEPT + np.random.normal(0, NOISE) for x in Xtrue] # np.random.normal skews our data \n",
    "\n",
    "plt.scatter(Xtrue, ytrue, s=0.5)\n",
    "plt.title('Random data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate a line\n",
    "Generate a random line of fit for baseline to improve it later with the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_line(x_data, slope, intercept):\n",
    "    return [slope * x + intercept for x in x_data]\n",
    "ypred = make_line(Xtrue, 4, 2)\n",
    "\n",
    "plt.scatter(Xtrue, ytrue, s=0.5)\n",
    "plt.plot(Xtrue, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loss function\n",
    "Calculate the loss function for the linear regression  \n",
    "Loss function: Mean Sqared Error (MSE)\n",
    "\n",
    "### MSE = $\\frac{1}{n}\\sum_{t=1}^{n}(y-\\hat{y})^2$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(ytrue, ypred):\n",
    "    return sum([(yt-yp)**2 for yt, yp in zip(ytrue, ypred) ]) / len(ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mse(ytrue, ypred)\n",
    "print(f'Slope of the MSE curve: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Gradient\n",
    "Now we calculate the gradient for the loss function curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_of_loss_function(Xtrue, ytrue, slope, intercept):\n",
    "    \n",
    "    step_size = 0.001\n",
    "    \n",
    "    # we want to make an initial linear regression line, to improve it later \n",
    "    first_line = make_line(Xtrue, slope, intercept)\n",
    "    \n",
    "    # tweak the slope\n",
    "    new_slope = slope + step_size\n",
    "    new_slope_line = make_line(Xtrue, new_slope, intercept)\n",
    "    \n",
    "    # tweak the intercept\n",
    "    new_intercept = intercept + step_size\n",
    "    new_intercept_line = make_line(Xtrue, slope, new_intercept)\n",
    "    \n",
    "    # calculate the gradient for those tweaks (change in y over the change in x).\n",
    "    new_slope_gradient = ((mse(ytrue, new_slope_line) - mse(ytrue, first_line)) / step_size)\n",
    "                          \n",
    "    new_intercept_gradient = ((mse(ytrue, new_intercept_line) - mse(ytrue, first_line)) / step_size)    \n",
    "    \n",
    "    # return the gradient\n",
    "    return new_slope_gradient, new_intercept_gradient\n",
    "    \n",
    "# this function only tweaks one variable at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_of_loss_function(Xtrue, ytrue, -4, 3) \n",
    "# these are the gradients of the new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Loop the process\n",
    "Now we can loop though a process of finding the lowest function score possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive matplotlib \n",
    "plt.ioff()\n",
    "\n",
    "# Add random starting parameters\n",
    "SLOPE = -5.5\n",
    "INTERCEPT = -3.0\n",
    "LEARNING_RATE = 0.08\n",
    "\n",
    "for i in range(500):\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    # Get the gradients\n",
    "    slope_gradient, intercept_gradient = gradient_of_loss_function(Xtrue, ytrue, SLOPE, INTERCEPT)\n",
    "\n",
    "    # Update our slope and intercept based on them\n",
    "    SLOPE_NEW = SLOPE - slope_gradient * LEARNING_RATE\n",
    "    INTERCEPT_NEW = INTERCEPT - intercept_gradient * LEARNING_RATE\n",
    "\n",
    "    # Reset our slope and intercept\n",
    "    SLOPE = SLOPE_NEW\n",
    "    INTERCEPT = INTERCEPT_NEW\n",
    "\n",
    "    # Prints only every 10th interation\n",
    "    if i % 10 == 0:\n",
    "        plt.scatter(Xtrue, ytrue, s=0.5)\n",
    "        plt.plot(Xtrue, make_line(Xtrue, SLOPE, INTERCEPT))\n",
    "        plt.title(f'gradient_descent_at_step_{i}')\n",
    "        plt.savefig(f'plots/step_{i}.png')\n",
    "        plt.figure()\n",
    "        print(f'The updated slope is : {SLOPE :8.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create gif\n",
    "And visualise our results using imagio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif():\n",
    "    images = []\n",
    "    for i in range(500):\n",
    "        if i % 10 == 0:\n",
    "            filename = f'plots/step_{i}.png'\n",
    "            image = imageio.imread(filename)\n",
    "            images.append(image)\n",
    "\n",
    "    imageio.mimsave('linear_gradient.gif', images, fps=15)\n",
    "    return imageio.mimsave('linear_gradient.gif', images, fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the slope converges to 2 (the true slope of our data) \n",
    "The goal is to get to what we assigned the slope in the beginning (We assigned 2.0)  \n",
    "With Linear regressions the number of iterations should be between 10 - 100  \n",
    "If we have scaled data we will reach the optimal point faster  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"linear_gradient.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
